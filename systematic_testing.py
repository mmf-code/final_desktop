#!/usr/bin/env python3
"""
Systematic Testing Script for Multi-Agent Formation Control
Tests various combinations of PID improvements, FLS, wind, and feed-forward control
"""

import subprocess
import os
import shutil
import time
from pathlib import Path
import yaml
from datetime import datetime

class SystematicTester:
    def __init__(self):
        self.base_config_path = "agent_control_pkg/config/simulation_params.yaml"
        self.external_config_path = "C:/Users/ataka/OneDrive/Masa√ºst√º/config/simulation_params.yaml"
        self.executable_path = "./build/Release/multi_drone_pid_tester.exe"
        self.results_dir = "systematic_test_results"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Create results directory
        Path(self.results_dir).mkdir(exist_ok=True)
        
    def update_config_value(self, config_path, key_path, value):
        """Update a specific configuration value using PowerShell"""
        ps_command = f"""
        $config = Get-Content "{config_path}" | ConvertFrom-Yaml
        $keys = "{key_path}".Split('.')
        $current = $config
        for ($i = 0; $i -lt $keys.Length - 1; $i++) {{
            $current = $current[$keys[$i]]
        }}
        $current[$keys[-1]] = {str(value).lower() if isinstance(value, bool) else value}
        $config | ConvertTo-Yaml | Set-Content "{config_path}"
        """
        subprocess.run(["powershell", "-Command", ps_command])
    
    def run_test(self, test_name, config_modifications):
        """Run a single test with specific configuration"""
        print(f"\nüî¨ Running Test: {test_name}")
        print("=" * 50)
        
        # Apply configuration modifications using PowerShell
        for key_path, value in config_modifications.items():
            if key_path == "controller_settings.fls.enable":
                ps_cmd = f'$config = Get-Content "{self.external_config_path}"; $flsLine = $config | Select-String -Pattern "enable:" | Select-Object -Index 1; if ($flsLine) {{ $lineNum = $flsLine.LineNumber - 1; $config[$lineNum] = "    enable: {str(value).lower()}"; $config | Set-Content "{self.external_config_path}" }}'
            elif key_path == "scenario_settings.enable_wind":
                ps_cmd = f'$config = Get-Content "{self.external_config_path}"; $windLine = $config | Select-String -Pattern "enable_wind:" | Select-Object -First 1; if ($windLine) {{ $lineNum = $windLine.LineNumber - 1; $config[$lineNum] = "  enable_wind: {str(value).lower()}"; $config | Set-Content "{self.external_config_path}" }}'
            elif key_path == "controller_settings.pid.enable_feedforward":
                ps_cmd = f'$config = Get-Content "{self.external_config_path}"; $ffLine = $config | Select-String -Pattern "enable_feedforward:" | Select-Object -First 1; if ($ffLine) {{ $lineNum = $ffLine.LineNumber - 1; $config[$lineNum] = "    enable_feedforward: {str(value).lower()}"; $config | Set-Content "{self.external_config_path}" }}'
            elif key_path == "controller_settings.pid.enable_derivative_filter":
                ps_cmd = f'$config = Get-Content "{self.external_config_path}"; $dfLine = $config | Select-String -Pattern "enable_derivative_filter:" | Select-Object -First 1; if ($dfLine) {{ $lineNum = $dfLine.LineNumber - 1; $config[$lineNum] = "    enable_derivative_filter: {str(value).lower()}"; $config | Set-Content "{self.external_config_path}" }}'
            else:
                continue  # Skip unsupported modifications for now
            
            subprocess.run(["powershell", "-Command", ps_cmd])
        
        print(f"Configuration applied: {config_modifications}")
        
        # Brief pause to ensure config is written
        time.sleep(1)
        
        # Run the test
        try:
            result = subprocess.run([self.executable_path], 
                                  capture_output=True, text=True, timeout=180)
            
            if result.returncode == 0:
                print("‚úÖ Test completed successfully")
                self.organize_results(test_name)
                return True
            else:
                print(f"‚ùå Test failed with return code: {result.returncode}")
                return False
                
        except subprocess.TimeoutExpired:
            print("‚è∞ Test timed out after 3 minutes")
            return False
        except Exception as e:
            print(f"üí• Test failed with exception: {e}")
            return False
    
    def organize_results(self, test_name):
        """Organize test results into structured directories"""
        test_dir = Path(self.results_dir) / f"{self.timestamp}_{test_name}"
        test_dir.mkdir(exist_ok=True)
        
        # Move CSV and metrics files
        output_dir = Path("simulation_outputs")
        if output_dir.exists():
            for file in output_dir.glob("*.csv"):
                if file.stat().st_mtime > time.time() - 180:  # Modified in last 3 minutes
                    shutil.copy2(file, test_dir / f"{test_name}_{file.name}")
            
            for file in output_dir.glob("*.txt"):
                if file.stat().st_mtime > time.time() - 180:  # Modified in last 3 minutes
                    shutil.copy2(file, test_dir / f"{test_name}_{file.name}")
        
        print(f"üìÅ Results saved to: {test_dir}")
    
    def run_feature_comparison_suite(self):
        """Run systematic feature comparison tests"""
        
        test_scenarios = [
            # Core Comparison Tests
            {
                "name": "01_Baseline_Clean",
                "config": {
                    "controller_settings.fls.enable": False,
                    "scenario_settings.enable_wind": False,
                    "controller_settings.pid.enable_feedforward": False,
                    "controller_settings.pid.enable_derivative_filter": True  # Keep critical fixes
                }
            },
            {
                "name": "02_FLS_Effect",
                "config": {
                    "controller_settings.fls.enable": True,
                    "scenario_settings.enable_wind": False,
                    "controller_settings.pid.enable_feedforward": False,
                    "controller_settings.pid.enable_derivative_filter": True
                }
            },
            {
                "name": "03_Wind_Challenge", 
                "config": {
                    "controller_settings.fls.enable": False,
                    "scenario_settings.enable_wind": True,
                    "controller_settings.pid.enable_feedforward": False,
                    "controller_settings.pid.enable_derivative_filter": True
                }
            },
            {
                "name": "04_FeedForward_Benefit",
                "config": {
                    "controller_settings.fls.enable": False,
                    "scenario_settings.enable_wind": False,
                    "controller_settings.pid.enable_feedforward": True,
                    "controller_settings.pid.enable_derivative_filter": True
                }
            },
            {
                "name": "05_FLS_vs_Wind",
                "config": {
                    "controller_settings.fls.enable": True,
                    "scenario_settings.enable_wind": True,
                    "controller_settings.pid.enable_feedforward": False,
                    "controller_settings.pid.enable_derivative_filter": True
                }
            },
            {
                "name": "06_Full_System_Optimal",
                "config": {
                    "controller_settings.fls.enable": True,
                    "scenario_settings.enable_wind": True,
                    "controller_settings.pid.enable_feedforward": True,
                    "controller_settings.pid.enable_derivative_filter": True
                }
            }
        ]
        
        print(f"üöÄ Starting Feature Comparison Suite ({len(test_scenarios)} tests)")
        print(f"üìÖ Timestamp: {self.timestamp}")
        print("=" * 60)
        
        results = {}
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nüìä Progress: {i}/{len(test_scenarios)}")
            success = self.run_test(scenario["name"], scenario["config"])
            results[scenario["name"]] = success
            time.sleep(2)  # Brief pause between tests
        
        self.generate_comparison_report(results)
        return results
    
    def generate_comparison_report(self, results):
        """Generate comparison analysis report"""
        report_path = Path(self.results_dir) / f"comparison_analysis_{self.timestamp}.md"
        
        with open(report_path, 'w') as f:
            f.write("# Multi-Agent Formation Control - Feature Comparison Analysis\n\n")
            f.write(f"**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Test ID:** {self.timestamp}\n\n")
            
            f.write("## Test Results Summary\n\n")
            f.write("| Test Name | Status | Description |\n")
            f.write("|-----------|--------|-------------|\n")
            
            descriptions = {
                "01_Baseline_Clean": "Basic PID with improvements, no FLS/Wind/FF",
                "02_FLS_Effect": "Baseline + Fuzzy Logic System",
                "03_Wind_Challenge": "Baseline + Wind Disturbances", 
                "04_FeedForward_Benefit": "Baseline + Feed-Forward Control",
                "05_FLS_vs_Wind": "FLS handling wind disturbances",
                "06_Full_System_Optimal": "All features enabled"
            }
            
            for test_name, success in results.items():
                status = "‚úÖ PASS" if success else "‚ùå FAIL"
                desc = descriptions.get(test_name, "Custom test")
                f.write(f"| {test_name} | {status} | {desc} |\n")
            
            f.write("\n## Analysis Instructions\n\n")
            f.write("### Performance Metrics to Compare:\n")
            f.write("1. **Overshoot Percentage** - Lower is better\n")
            f.write("2. **Settling Time (2% and 5%)** - Faster is better\n") 
            f.write("3. **Steady-State Error** - Lower is better\n")
            f.write("4. **Wind Disturbance Rejection** - Compare wind vs no-wind tests\n\n")
            
            f.write("### Key Comparisons:\n")
            f.write("- **FLS Effectiveness**: Compare Test 01 vs Test 02\n")
            f.write("- **Wind Impact**: Compare Test 01 vs Test 03\n") 
            f.write("- **Feed-Forward Benefit**: Compare Test 01 vs Test 04\n")
            f.write("- **FLS vs Wind**: Test 05 shows FLS handling disturbances\n")
            f.write("- **Complete System**: Test 06 shows optimal performance\n\n")
            
            f.write("### Next Steps:\n")
            f.write("1. Run analysis scripts on the generated CSV data\n")
            f.write("2. Generate comparative plots\n")
            f.write("3. Calculate improvement percentages\n")
            f.write("4. Document findings for final report\n")
        
        print(f"\nüìã Comparison analysis generated: {report_path}")

if __name__ == "__main__":
    tester = SystematicTester()
    
    print("üéØ Multi-Agent Formation Control - Systematic Testing")
    print("=" * 55)
    print("This will run systematic tests to compare:")
    print("‚Ä¢ Baseline PID vs FLS enhancement")  
    print("‚Ä¢ System performance with/without wind")
    print("‚Ä¢ Feed-forward control benefits")
    print("‚Ä¢ Combined system effectiveness")
    print()
    
    try:
        confirm = input("Start systematic testing? (y/n): ").strip().lower()
        
        if confirm == 'y' or confirm == 'yes':
            print("\nüöÄ Starting systematic test suite...")
            results = tester.run_feature_comparison_suite()
            
            # Generate summary
            successful_tests = sum(results.values())
            total_tests = len(results)
            
            print(f"\nüéâ Testing Complete!")
            print(f"‚úÖ Successful: {successful_tests}/{total_tests}")
            print(f"üìÅ Results saved in: {tester.results_dir}")
            print("üìä Use the generated CSV files for detailed analysis")
            
        else:
            print("Testing cancelled.")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Testing interrupted by user")
    except Exception as e:
        print(f"\nüí• Testing failed: {e}") 