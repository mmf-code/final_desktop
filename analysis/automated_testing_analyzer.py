#!/usr/bin/env python3
"""
Automated Testing Results Analyzer
Multi-Agent Formation Control with PID, FLS, and Wind Disturbances
Specifically designed for automated_testing_* folder structure
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import warnings
import seaborn as sns
from datetime import datetime
import re
import os
warnings.filterwarnings('ignore')

# Set professional matplotlib style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 11
plt.rcParams['figure.titlesize'] = 16
plt.rcParams['lines.linewidth'] = 2
plt.rcParams['grid.alpha'] = 0.3

# Color schemes
COLORS = {
    'Conservative': '#2E8B57',  # Sea Green
    'Optimal': '#1f77b4',       # Blue
    'Aggressive': '#d62728',    # Red
    'FLS': '#ff7f0e',          # Orange
    'Wind': '#9467bd',         # Purple
    'Full': '#17becf'          # Cyan
}

DRONE_COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']


class AutomatedTestingAnalyzer:
    def __init__(self, base_results_path="final_project_results"):
        self.base_path = Path(base_results_path)
        self.latest_results_path = None
        self.test_data = {}
        self.metrics_data = {}

        # Find latest automated testing results
        self.find_latest_results()

        # Load all test data
        if self.latest_results_path:
            self.load_test_data()

    def find_latest_results(self):
        """Find the most recent automated_testing_* folder"""
        pattern = "automated_testing_*"
        test_dirs = list(self.base_path.glob(pattern))

        if not test_dirs:
            print(f"‚ùå No automated testing directories found in {self.base_path}")
            return

        # Sort by creation time (newest first)
        test_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        self.latest_results_path = test_dirs[0]

        print(f"üìÅ Found latest results: {self.latest_results_path.name}")
        print(f"üìÖ Created: {datetime.fromtimestamp(self.latest_results_path.stat().st_mtime)}")

    def load_test_data(self):
        """Load CSV and metrics data from all test directories"""
        if not self.latest_results_path:
            print("‚ùå No results path available")
            return

        test_dirs = [d for d in self.latest_results_path.iterdir() if d.is_dir()]
        print(f"üìä Found {len(test_dirs)} test directories")

        for test_dir in sorted(test_dirs):
            test_name = test_dir.name.split('_', 1)[1] if '_' in test_dir.name else test_dir.name

            # Load CSV files
            csv_files = list(test_dir.glob("*.csv"))
            if csv_files:
                # Get the most recent CSV file for this test
                latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
                try:
                    df = pd.read_csv(latest_csv)
                    self.test_data[test_name] = df
                    print(f"‚úÖ Loaded {test_name}: {len(df)} data points from {latest_csv.name}")
                except Exception as e:
                    print(f"‚ùå Error loading {latest_csv}: {e}")

            # Load metrics files
            metrics_files = list(test_dir.glob("*metrics*.txt"))
            if metrics_files:
                latest_metrics = max(metrics_files, key=lambda x: x.stat().st_mtime)
                try:
                    self.metrics_data[test_name] = self.parse_metrics_file(latest_metrics)
                except Exception as e:
                    print(f"‚ùå Error loading metrics {latest_metrics}: {e}")

    def parse_metrics_file(self, metrics_file):
        """Parse the metrics text file to extract performance data"""
        metrics = {}
        with open(metrics_file, 'r') as f:
            content = f.read()

        # Extract PID parameters
        kp_match = re.search(r'Kp=([0-9.]+)', content)
        ki_match = re.search(r'Ki=([0-9.]+)', content)
        kd_match = re.search(r'Kd=([0-9.]+)', content)

        if kp_match and ki_match and kd_match:
            metrics['PID'] = {
                'Kp': float(kp_match.group(1)),
                'Ki': float(ki_match.group(1)),
                'Kd': float(kd_match.group(1))
            }

        # Extract FLS and Wind status
        metrics['FLS'] = 'FLS: ON' in content
        metrics['Wind'] = 'Wind: ON' in content

        # Extract performance metrics for each phase
        phases = re.findall(r'-- METRICS FOR PHASE (\d+) --(.*?)(?=-- METRICS FOR PHASE|\Z)', content, re.DOTALL)
        metrics['phases'] = {}

        for phase_num, phase_content in phases:
            phase_data = {}

            # Extract overshoot, settling time, steady-state error for each drone
            drone_sections = re.findall(r'Drone (\d+):(.*?)(?=Drone \d+:|\Z)', phase_content, re.DOTALL)
            phase_data['drones'] = {}

            for drone_num, drone_content in drone_sections:
                drone_data = {}

                # Extract X-axis metrics
                x_metrics = re.search(r'X-axis: OS=([0-9.]+)%, ST\(2%\)=([0-9.]+)s, ST\(5%\)=([0-9.]+)s, SSE=([0-9.]+)%', drone_content)
                if x_metrics:
                    drone_data['X'] = {
                        'overshoot': float(x_metrics.group(1)),
                        'settling_time_2pct': float(x_metrics.group(2)),
                        'settling_time_5pct': float(x_metrics.group(3)),
                        'steady_state_error': float(x_metrics.group(4))
                    }

                # Extract Y-axis metrics
                y_metrics = re.search(r'Y-axis: OS=([0-9.]+)%, ST\(2%\)=([0-9.]+)s, ST\(5%\)=([0-9.]+)s, SSE=([0-9.]+)%', drone_content)
                if y_metrics:
                    drone_data['Y'] = {
                        'overshoot': float(y_metrics.group(1)),
                        'settling_time_2pct': float(y_metrics.group(2)),
                        'settling_time_5pct': float(y_metrics.group(3)),
                        'steady_state_error': float(y_metrics.group(4))
                    }

                phase_data['drones'][f'Drone_{drone_num}'] = drone_data

            metrics['phases'][f'Phase_{phase_num}'] = phase_data

        return metrics

    def plot_trajectory_comparison(self):
        """Plot trajectory comparison for all tests"""
        if not self.test_data:
            print("‚ùå No test data available")
            return

        print("üìà Generating trajectory comparison plots...")

        # Create subplots for trajectory comparison
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Multi-Agent Formation Control: Trajectory Comparison\nAutomated Testing Results',
                     fontsize=16, fontweight='bold')

        # Plot 1: Conservative vs Optimal vs Aggressive PID
        ax1 = axes[0, 0]
        pid_tests = ['Conservative_PID', 'Optimal_PID', 'Aggressive_PID']

        for test_name in pid_tests:
            if test_name in self.test_data:
                data = self.test_data[test_name]
                if 'CurrentX0' in data.columns and 'CurrentY0' in data.columns:
                    color = COLORS.get(test_name.split('_')[0], '#000000')
                    ax1.plot(data['CurrentX0'], data['CurrentY0'],
                            color=color, linewidth=2, label=test_name.replace('_', ' '), alpha=0.8)

                    # Plot target trajectory
                    if 'TargetX0' in data.columns and 'TargetY0' in data.columns:
                        ax1.plot(data['TargetX0'], data['TargetY0'],
                                '--k', alpha=0.5, linewidth=1, label='Target' if test_name == pid_tests[0] else "")

        ax1.set_xlabel('X Position (m)')
        ax1.set_ylabel('Y Position (m)')
        ax1.set_title('PID Tuning Comparison')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        ax1.axis('equal')

        # Plot 2: Effect of FLS
        ax2 = axes[0, 1]
        fls_comparison = [('Conservative_PID', 'FLS_Conservative'), ('Optimal_PID', 'FLS_Optimal')]

        for base_test, fls_test in fls_comparison:
            if base_test in self.test_data and fls_test in self.test_data:
                # Base PID
                data_base = self.test_data[base_test]
                if 'CurrentX0' in data_base.columns:
                    ax2.plot(data_base['CurrentX0'], data_base['CurrentY0'],
                            color=COLORS.get(base_test.split('_')[0], '#000000'),
                            linewidth=2, label=f'{base_test.split("_")[0]} PID Only', alpha=0.7, linestyle='-')

                # With FLS
                data_fls = self.test_data[fls_test]
                if 'CurrentX0' in data_fls.columns:
                    ax2.plot(data_fls['CurrentX0'], data_fls['CurrentY0'],
                            color=COLORS.get(base_test.split('_')[0], '#000000'),
                            linewidth=2, label=f'{base_test.split("_")[0]} + FLS', alpha=0.9, linestyle='--')

        ax2.set_xlabel('X Position (m)')
        ax2.set_ylabel('Y Position (m)')
        ax2.set_title('Effect of Fuzzy Logic System (FLS)')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        ax2.axis('equal')

        # Plot 3: Effect of Wind Disturbances
        ax3 = axes[1, 0]
        wind_comparison = [('Conservative_PID', 'Wind_Conservative'), ('Optimal_PID', 'Wind_Optimal')]

        for base_test, wind_test in wind_comparison:
            if base_test in self.test_data and wind_test in self.test_data:
                # No wind
                data_base = self.test_data[base_test]
                if 'CurrentX0' in data_base.columns:
                    ax3.plot(data_base['CurrentX0'], data_base['CurrentY0'],
                            color=COLORS.get(base_test.split('_')[0], '#000000'),
                            linewidth=2, label=f'{base_test.split("_")[0]} No Wind', alpha=0.7, linestyle='-')

                # With wind
                data_wind = self.test_data[wind_test]
                if 'CurrentX0' in data_wind.columns:
                    ax3.plot(data_wind['CurrentX0'], data_wind['CurrentY0'],
                            color=COLORS.get(base_test.split('_')[0], '#000000'),
                            linewidth=2, label=f'{base_test.split("_")[0]} + Wind', alpha=0.9, linestyle=':')

        ax3.set_xlabel('X Position (m)')
        ax3.set_ylabel('Y Position (m)')
        ax3.set_title('Effect of Wind Disturbances')
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        ax3.axis('equal')

        # Plot 4: Full System Comparison
        ax4 = axes[1, 1]
        full_tests = ['Conservative_PID', 'FLS_Conservative', 'Wind_Conservative', 'Full_Conservative']

        for test_name in full_tests:
            if test_name in self.test_data:
                data = self.test_data[test_name]
                if 'CurrentX0' in data.columns:
                    color = COLORS.get(test_name.split('_')[0], COLORS.get(test_name.split('_')[1], '#000000'))
                    ax4.plot(data['CurrentX0'], data['CurrentY0'],
                            color=color, linewidth=2, label=test_name.replace('_', ' '), alpha=0.8)

        ax4.set_xlabel('X Position (m)')
        ax4.set_ylabel('Y Position (m)')
        ax4.set_title('Progressive System Enhancement')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        ax4.axis('equal')

        plt.tight_layout()

        # Save plot
        output_path = self.latest_results_path / "trajectory_comparison.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Saved trajectory comparison: {output_path}")
        plt.show()

    def plot_error_analysis(self):
        """Plot tracking error analysis"""
        if not self.test_data:
            print("‚ùå No test data available")
            return

        print("üìä Generating error analysis plots...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Formation Control Error Analysis\nAutomated Testing Results',
                     fontsize=16, fontweight='bold')

        # Plot 1: Error magnitude over time
        ax1 = axes[0, 0]
        for test_name, data in self.test_data.items():
            if 'ErrorX0' in data.columns and 'ErrorY0' in data.columns:
                error_magnitude = np.sqrt(data['ErrorX0']**2 + data['ErrorY0']**2)
                color = self.get_test_color(test_name)
                ax1.plot(data['Time'], error_magnitude,
                        color=color, linewidth=2, label=test_name.replace('_', ' '), alpha=0.8)

        ax1.set_xlabel('Time (s)')
        ax1.set_ylabel('Position Error Magnitude (m)')
        ax1.set_title('Position Tracking Error Over Time')
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # Plot 2: RMS error comparison
        ax2 = axes[0, 1]
        test_names = []
        rms_errors = []

        for test_name, data in self.test_data.items():
            if 'ErrorX0' in data.columns and 'ErrorY0' in data.columns:
                error_magnitude = np.sqrt(data['ErrorX0']**2 + data['ErrorY0']**2)
                rms_error = np.sqrt(np.mean(error_magnitude**2))
                test_names.append(test_name.replace('_', ' '))
                rms_errors.append(rms_error)

        if test_names:
            colors = [self.get_test_color(name.replace(' ', '_')) for name in test_names]
            bars = ax2.bar(range(len(test_names)), rms_errors, color=colors, alpha=0.8)
            ax2.set_xlabel('Test Configuration')
            ax2.set_ylabel('RMS Error (m)')
            ax2.set_title('RMS Tracking Error Comparison')
            ax2.set_xticks(range(len(test_names)))
            ax2.set_xticklabels(test_names, rotation=45, ha='right')
            ax2.grid(True, alpha=0.3)

            # Add value labels on bars
            for bar, value in zip(bars, rms_errors):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=9)

        # Plot 3: Error over time for key tests
        ax3 = axes[1, 0]
        key_tests = ['Conservative_PID', 'Optimal_PID', 'FLS_Optimal', 'Wind_Optimal']
        for test_name in key_tests:
            if test_name in self.test_data:
                data = self.test_data[test_name]
                if 'ErrorX0' in data.columns and 'ErrorY0' in data.columns:
                    error_magnitude = np.sqrt(data['ErrorX0']**2 + data['ErrorY0']**2)
                    color = self.get_test_color(test_name)
                    ax3.plot(data['Time'], error_magnitude,
                            color=color, linewidth=2, label=test_name.replace('_', ' '), alpha=0.8)

        ax3.set_xlabel('Time (s)')
        ax3.set_ylabel('Position Error Magnitude (m)')
        ax3.set_title('Key Configuration Comparison')
        ax3.grid(True, alpha=0.3)
        ax3.legend()

        # Plot 4: Max error comparison
        ax4 = axes[1, 1]
        test_names = []
        max_errors = []

        for test_name, data in self.test_data.items():
            if 'ErrorX0' in data.columns and 'ErrorY0' in data.columns:
                error_magnitude = np.sqrt(data['ErrorX0']**2 + data['ErrorY0']**2)
                max_error = np.max(error_magnitude)
                test_names.append(test_name.replace('_', ' '))
                max_errors.append(max_error)

        if test_names:
            colors = [self.get_test_color(name.replace(' ', '_')) for name in test_names]
            bars = ax4.bar(range(len(test_names)), max_errors, color=colors, alpha=0.8)
            ax4.set_xlabel('Test Configuration')
            ax4.set_ylabel('Maximum Error (m)')
            ax4.set_title('Maximum Tracking Error Comparison')
            ax4.set_xticks(range(len(test_names)))
            ax4.set_xticklabels(test_names, rotation=45, ha='right')
            ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        # Save plot
        output_path = self.latest_results_path / "error_analysis.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Saved error analysis: {output_path}")
        plt.show()

    def get_test_color(self, test_name):
        """Get appropriate color for test configuration"""
        if 'Conservative' in test_name:
            return COLORS['Conservative']
        elif 'Optimal' in test_name:
            return COLORS['Optimal']
        elif 'Aggressive' in test_name:
            return COLORS['Aggressive']
        elif 'FLS' in test_name:
            return COLORS['FLS']
        elif 'Wind' in test_name:
            return COLORS['Wind']
        elif 'Full' in test_name:
            return COLORS['Full']
        else:
            return '#666666'

    def generate_summary_report(self):
        """Generate a comprehensive summary report"""
        if not self.latest_results_path:
            print("‚ùå No results path available")
            return

        print("üìã Generating summary report...")

        report_path = self.latest_results_path / "analysis_summary_report.txt"

        with open(report_path, 'w') as f:
            f.write("AUTOMATED TESTING ANALYSIS SUMMARY REPORT\n")
            f.write("=" * 60 + "\n\n")

            f.write(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Results Directory: {self.latest_results_path.name}\n")
            f.write(f"Total Tests Analyzed: {len(self.test_data)}\n\n")

            # Test configurations summary
            f.write("TEST CONFIGURATIONS ANALYZED:\n")
            f.write("-" * 40 + "\n")
            for i, test_name in enumerate(sorted(self.test_data.keys()), 1):
                f.write(f"{i:2d}. {test_name.replace('_', ' ')}\n")
                if test_name in self.metrics_data:
                    metrics = self.metrics_data[test_name]
                    if 'PID' in metrics:
                        pid = metrics['PID']
                        f.write(f"    PID: Kp={pid['Kp']}, Ki={pid['Ki']}, Kd={pid['Kd']}\n")
                    f.write(f"    FLS: {'ON' if metrics.get('FLS', False) else 'OFF'}\n")
                    f.write(f"    Wind: {'ON' if metrics.get('Wind', False) else 'OFF'}\n")
                f.write("\n")

            # RMS error analysis
            f.write("RMS TRACKING ERROR ANALYSIS:\n")
            f.write("-" * 40 + "\n")
            rms_data = []
            for test_name, data in self.test_data.items():
                if 'ErrorX0' in data.columns and 'ErrorY0' in data.columns:
                    error_magnitude = np.sqrt(data['ErrorX0']**2 + data['ErrorY0']**2)
                    rms_error = np.sqrt(np.mean(error_magnitude**2))
                    rms_data.append((test_name, rms_error))

            # Sort by RMS error
            rms_data.sort(key=lambda x: x[1])

            for i, (test_name, rms_error) in enumerate(rms_data, 1):
                f.write(f"{i:2d}. {test_name.replace('_', ' '):<25} {rms_error:.4f} m\n")

            f.write("\n")

            # Recommendations
            f.write("ANALYSIS INSIGHTS AND RECOMMENDATIONS:\n")
            f.write("-" * 50 + "\n")

            if rms_data:
                best_test = rms_data[0][0]
                worst_test = rms_data[-1][0]
                f.write(f"‚Ä¢ Best performing configuration: {best_test.replace('_', ' ')}\n")
                f.write(f"‚Ä¢ Highest error configuration: {worst_test.replace('_', ' ')}\n")
                improvement = (rms_data[-1][1] - rms_data[0][1]) / rms_data[-1][1] * 100
                f.write(f"‚Ä¢ Performance improvement: {improvement:.1f}% reduction in RMS error\n\n")

            f.write("‚Ä¢ Consider the trade-offs between settling time, overshoot, and steady-state error\n")
            f.write("‚Ä¢ FLS integration shows benefits in disturbance rejection\n")
            f.write("‚Ä¢ Wind disturbances significantly affect trajectory tracking\n")
            f.write("‚Ä¢ Feed-forward control can improve transient response\n")

        print(f"üìÑ Summary report saved: {report_path}")

    def run_complete_analysis(self):
        """Run all analysis functions"""
        if not self.test_data:
            print("‚ùå No test data loaded. Cannot run analysis.")
            return

        print("üöÄ Starting complete automated testing analysis...")
        print(f"üìÅ Analyzing results from: {self.latest_results_path.name}")

        # Generate all plots
        self.plot_trajectory_comparison()
        self.plot_error_analysis()

        # Generate summary report
        self.generate_summary_report()

        print("\n‚úÖ Analysis complete! Check the results directory for:")
        print(f"   üìä Plots: trajectory_comparison.png, error_analysis.png")
        print(f"   üìÑ Report: analysis_summary_report.txt")
        print(f"   üìÅ Location: {self.latest_results_path}")


def main():
    """Main function to run the analyzer"""
    print("üéØ Multi-Agent Formation Control - Automated Testing Analyzer")
    print("=" * 65)

    # Initialize analyzer
    analyzer = AutomatedTestingAnalyzer()

    if not analyzer.test_data:
        print("\n‚ùå No test data found. Please run automated tests first:")
        print("   PowerShell: .\\run_simulations_enhanced.ps1 -RunAll")
        return

    # Run complete analysis
    analyzer.run_complete_analysis()

    print("\nüéâ Analysis completed successfully!")
    print("Use the generated plots and reports for your thesis research.")


if __name__ == "__main__":
    main()
